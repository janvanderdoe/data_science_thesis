{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "The goal of this code is the clean the raw data and to merge it with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data with decimals as dot\n",
    "df = pd.read_csv('../../data/amazon_search_hedonic.csv', sep = \";\", decimal = \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates and empty variables\n",
    "df1 = len(df)\n",
    "print(df1)\n",
    "df = df.drop_duplicates(subset = ['review_title', 'review', 'date', 'name', 'profile_url'])\n",
    "df2 = len(df)\n",
    "print(df2)\n",
    "print('Removed', df1 - df2, 'duplicates')\n",
    "df = df[df['country'] == 'the United States']\n",
    "df3 = len(df)\n",
    "print('Removed', df3 - df2, 'non USA reviews')\n",
    "print(df3)\n",
    "df = df[df['review'] != '']\n",
    "df4 = len(df)\n",
    "print('Removed', df4 - df3, 'empty reviews')\n",
    "#remove reviews with na\n",
    "df = df.dropna(subset = ['review'])\n",
    "df5 = len(df)\n",
    "print('Removed', df5 - df4, 'na reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct mistake in encoding numerical data\n",
    "df['helpful'] = df['helpful'].str.replace(',', '')\n",
    "\n",
    "df['helpful'] = pd.to_numeric(df['helpful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute 0 for missing helpful votes (these are not truly missing, the scraper just didn't pick 0 votes up)\n",
    "df['helpful'] = df['helpful'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add id so can reorder into original order for concatting and merging\n",
    "df['id'] = range(0, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with category data\n",
    "def remove_last_digit(string):\n",
    "    return re.sub(r'\\d+$', '', string)\n",
    "\n",
    "df['asin_url_clean'] = df['asin_url'].apply(remove_last_digit)\n",
    "links_data = pd.read_csv(f\"../../gen/input/amazon_links_per_category.csv\", delimiter = \";\")\n",
    "#remove duplicates for links_data product_link\n",
    "links_data = links_data.drop_duplicates(subset = ['product_link'])\n",
    "df = pd.merge(df, links_data, left_on = 'asin_url_clean', right_on = 'product_link', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "df = df.drop(columns = ['brand', 'model', 'first_spec', 'asin_url_clean', 'product_link', 'page_link', 'name_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def count_photos(pics):\n",
    "    if pics == 'no':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(ast.literal_eval(pics))\n",
    "\n",
    "df['n_pictures'] = df['picture'].apply(count_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of review\n",
    "df['review_len'] = df['review'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%B %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_diff'] = (pd.to_datetime(df['scrape_date']) - df['date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of observations per category, and the number of unique products, and the number of non=nan profile values\n",
    "df_descriptives = df.groupby('category').agg({'category': 'size',\n",
    "                                  'asin': 'nunique',\n",
    "                                  'profile': lambda x: sum(~x.isna()),\n",
    "                                  'picture': lambda x: sum(x != 'no'),\n",
    "                                  'n_pictures': 'mean',\n",
    "                                  'review_len' : 'mean',\n",
    "                                  'helpful' : 'mean',\n",
    "                                    'rating' : 'mean',\n",
    "                                    #add average date\n",
    "                                    'days_diff' : 'mean'\n",
    "\n",
    "                                  })\n",
    "#df_descriptives.columns = ['n_obs', 'n_products', 'n_profiles', 'n_pictures', 'photos_per_review', 'helpful_mean', 'rating_mean']\n",
    "df_descriptives['profile'] = df_descriptives['profile']/df_descriptives['category']*100\n",
    "df_descriptives['picture'] = df_descriptives['picture']/df_descriptives['category']*100\n",
    "\n",
    "#change order of category\n",
    "df_descriptives = df_descriptives.reindex(df['category'].unique())\n",
    "#add average at bottom\n",
    "df_descriptives.loc['average'] = df_descriptives.mean()\n",
    "#sum at the bottom, except for the average row\n",
    "df_descriptives.loc['sum'] = df_descriptives.iloc[:-1, :].sum()\n",
    "#round values to 3 decimals\n",
    "df_descriptives = df_descriptives.round(2)\n",
    "#to percentage\n",
    "df_descriptives['profile'] = df_descriptives['profile'].astype(str) + '%'\n",
    "df_descriptives['picture'] = df_descriptives['picture'].astype(str) + '%'\n",
    "print(df_descriptives)\n",
    "#save to excel\n",
    "df_descriptives.to_excel('../../gen/output/amazon_descriptives.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of reviews with at least 1 helpful vote\n",
    "len(df[df['helpful'] > 0]) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK NLTK VERSION\n",
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation from review\n",
    "print(df['review'][0])\n",
    "df['review_clean'] = df['review'].str.replace('[^\\w\\s]','')\n",
    "print(df['review_clean'][0])\n",
    "#lowercase everything\n",
    "df['review_clean'] = df['review_clean'].str.lower()\n",
    "print(df['review_clean'][0])\n",
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['review_clean'] = df['review_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df['review_clean'][0])\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "df['review_clean'] = df['review_clean'].apply(lambda x: ' '.join([porter.stem(word) for word in x.split()]))\n",
    "print(df['review_clean'][0])\n",
    "#tokenize review\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#df['review_clean'] = df['review_clean'].apply(word_tokenize)\n",
    "#print(df['review_clean'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedonic = 3*[\"Hedonic\"] + 3*['Utilitarian'] + 3*[\"Hedonic\"] + 3*['Utilitarian']\n",
    "search = 6*[\"Search\"] + 6*[\"Experience\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.DataFrame({'category': df['category'].unique(), 'hedonic_utilitarian': hedonic, 'search_experience': search})\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with data\n",
    "df = pd.merge(df, categories, on = 'category', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace underscore with space and capitalize every word in category\n",
    "df['category'] = df['category'].str.replace('_', ' ').str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df.to_csv('../../gen/input/amazon_search_hedonic_clean.csv', sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge with api data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../gen/input/amazon_search_hedonic_clean.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(review):\n",
    "    return len(review.split(' '))\n",
    "\n",
    "df['wordcount'] = df['review'].apply(get_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data = pd.read_csv('../../gen/input/api_info.csv', sep = \";\")\n",
    "api_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make empty dataframe with same column names\n",
    "api_data_clean = pd.DataFrame(columns = api_data.columns)\n",
    "\n",
    "#loop over asins\n",
    "for asin in df['asin'].unique():\n",
    "    #get the data for the asin\n",
    "    asin_data = api_data[api_data['asin'] == asin]\n",
    "    #set datetime index\n",
    "    asin_data.index = pd.to_datetime(asin_data['date'])\n",
    "    asin_data = asin_data.interpolate(method='time')\n",
    "    asin_data = asin_data.fillna(method='bfill')\n",
    "    asin_data = asin_data.fillna(method='ffill')\n",
    "    #add to the dataframe\n",
    "    api_data_clean = api_data_clean.append(asin_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save api clean\n",
    "api_data_clean.to_csv('../../gen/input/api_info_clean.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open api clean\n",
    "api_data_clean = pd.read_csv('../../gen/input/api_info_clean.csv', sep = \";\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_features(asin):\n",
    "    df_agg = api_data_clean[api_data_clean['asin'] == asin][['price_new', 'rating']].agg(['mean', 'min', 'max', 'median'])\n",
    "    reshaped_df = pd.DataFrame(df_agg.values.reshape(1, -1))\n",
    "    reshaped_df.columns = [f'{col}_{idx}' for idx in df_agg.index for col in df_agg.columns]\n",
    "    reshaped_df['asin'] = asin\n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_clean[api_data_clean['asin'] == \"fdsaf\"][['price_new', 'rating']].agg(['mean', 'min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_info = pd.DataFrame(columns = ['price_new_mean', 'rating_mean', 'price_new_min', 'rating_min',\n",
    "       'price_new_max', 'rating_max', 'price_new_median', 'rating_median',\n",
    "       'asin'])\n",
    "for asin in df['asin'].unique():\n",
    "    asin_info = pd.concat([asin_info, return_features(asin)], axis = 0)\n",
    "\n",
    "asin_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_info.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge asin_info\n",
    "df = pd.merge(df, asin_info, on = 'asin', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date to datetime\n",
    "api_data_clean['date'] = pd.to_datetime(api_data_clean['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_clean.rename({'price_new' : 'price_rd', 'rating' : 'rating_rd', 'reviews' : 'reviews_rd'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df with api_data_clean on asin and nearest date\n",
    "df_m = pd.merge_asof(df.sort_values('date'), api_data_clean.sort_values('date'), on = 'date', by = 'asin', direction = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m['scrape_date'] = pd.to_datetime(df_m['scrape_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_clean_sd = api_data_clean.drop(columns = ['price_used', 'oos_new', 'oos_used', 'listed_since'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_clean_sd.rename({'price_rd' : 'price_sd', 'rating_rd' : 'rating_sd', 'reviews_rd' : 'reviews_sd', 'date' : 'scrape_date'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.merge_asof(df_m.sort_values('scrape_date'), api_data_clean_sd.sort_values('scrape_date'), on = 'scrape_date', by = 'asin', direction = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order df_m based on id\n",
    "df_m = df_m.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m['price_diff'] = df_m['price_sd'] - df_m['price_rd']\n",
    "df_m['rating_diff'] = df_m['rating_sd'] - df_m['rating_rd']\n",
    "df_m['reviews_diff'] = df_m['reviews_sd'] - df_m['reviews_rd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in reviews na using mean on review date\n",
    "df_m['price_rd'] = df_m['price_rd'].fillna(df_m['price_rd'].mean())\n",
    "df_m['rating_rd'] = df_m['rating_rd'].fillna(df_m['rating_rd'].mean())\n",
    "df_m['reviews_rd'] = df_m['reviews_rd'].fillna(df_m['reviews_rd'].mean())\n",
    "\n",
    "#varaibles at scrape date\n",
    "df_m['price_sd'] = df_m['price_sd'].fillna(df_m['price_sd'].mean())\n",
    "df_m['rating_sd'] = df_m['rating_sd'].fillna(df_m['rating_sd'].mean())\n",
    "df_m['reviews_sd'] = df_m['reviews_sd'].fillna(df_m['reviews_sd'].mean())\n",
    "\n",
    "#difference\n",
    "df_m['price_diff'] = df_m['price_diff'].fillna(df_m['price_diff'].mean())\n",
    "df_m['rating_diff'] = df_m['rating_diff'].fillna(df_m['rating_diff'].mean())\n",
    "df_m['reviews_diff'] = df_m['reviews_diff'].fillna(df_m['reviews_diff'].mean())\n",
    "\n",
    "#fill in for price and rating\n",
    "df_m['price_new_mean'] = df_m['price_new_mean'].fillna(df_m['price_new_mean'].mean())\n",
    "df_m['rating_mean'] = df_m['rating_mean'].fillna(df_m['rating_mean'].mean())\n",
    "df_m['price_new_min'] = df_m['price_new_min'].fillna(df_m['price_new_min'].mean())\n",
    "df_m['rating_min'] = df_m['rating_min'].fillna(df_m['rating_min'].mean())\n",
    "df_m['price_new_max'] = df_m['price_new_max'].fillna(df_m['price_new_max'].mean())\n",
    "df_m['rating_max'] = df_m['rating_max'].fillna(df_m['rating_max'].mean())\n",
    "df_m['price_new_median'] = df_m['price_new_median'].fillna(df_m['price_new_median'].mean())\n",
    "df_m['rating_median'] = df_m['rating_median'].fillna(df_m['rating_median'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df_m = df_m.rename(columns = {'name_x' : 'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save csv\n",
    "df_m.to_csv('../../gen/output/amazon_search_hedonic_clean_merged.csv', sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open df_m\n",
    "df_m = pd.read_csv('../../gen/output/amazon_search_hedonic_clean_merged.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select helpful versus unhelpful reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column that indicates if a review as 0 helpful votes, or more than 0\n",
    "df_m['helpful_cat'] = 0\n",
    "df_m.loc[df_m['helpful'] > 0, 'helpful_cat'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = df_m[df_m['helpful_cat'] == 1].shape[0]\n",
    "df_m_0 = df_m[df_m['helpful_cat'] == 0].sample(sample_size, random_state = 42, replace = False)\n",
    "df_m_0_1 = pd.concat([df_m_0, df_m[df_m['helpful_cat'] == 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort index\n",
    "df_m_0_1 = df_m_0_1.sort_index()\n",
    "df_m_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df_m_0_1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "df_m_0_1.to_csv('../../gen/output/balanced_dataset.csv', sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_0_1[['review', 'helpful']].to_csv('../../gen/output/balanced_review_helpful.csv', sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize and select training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.read_csv('../../gen/output/balanced_dataset.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a frequency table for reviews_diff with buckets of 10000\n",
    "pd.cut(df_m['reviews_diff'], bins = np.arange(-10000, 10000, 1000)).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index df_m\n",
    "df_m = df_m.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m['rating_deviation_sd'] = df_m['rating'] - df_m['rating_sd']\n",
    "\n",
    "df_m['rating_deviation_rd'] = df_m['rating'] - df_m['rating_rd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection = df_m[['helpful', 'review', 'days_diff', 'rating', 'hedonic_utilitarian', 'search_experience', 'video',\n",
    "                     'price_sd', 'price_diff', 'rating_diff', 'rating_deviation_sd', 'reviews_sd', 'reviews_diff',\n",
    "                     'n_pictures', 'review_len',\n",
    "                     'price_new_mean', 'rating_mean', 'price_new_min', 'rating_min', 'price_new_max', 'rating_max', 'price_new_median', 'rating_median',\n",
    "                     'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection['hedonic'] = df_selection['hedonic_utilitarian'].map({'Hedonic' : 1, 'Utilitarian' : 0})\n",
    "df_selection['experience'] = df_selection['search_experience'].map({'Experience' : 1, 'Search' : 0})\n",
    "df_selection['has_video'] = np.where(df_selection['video'].notna(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dummies from the category variable\n",
    "dummies = pd.get_dummies(df_selection['category'], drop_first=True)\n",
    "df_selection= pd.concat([df_selection, dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection = df_selection.drop(columns = ['hedonic_utilitarian',\n",
    "       'search_experience', 'video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection = df_selection.drop(columns = ['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection.to_csv('../../gen/output/balanced_selection_variables.csv', sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection[['helpful', 'review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat df_normalized with reviews and helpful\n",
    "df_normalized = pd.concat([df_normalized, df_selection[['helpful', 'review']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.concat([df_normalized, df_selection[['hedonic', 'experience', 'has_video']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make correlation matrix of df_normalized\n",
    "df_normalized.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df_normalized.to_csv('../../gen/output/normalized_selection.csv', sep = \";\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
