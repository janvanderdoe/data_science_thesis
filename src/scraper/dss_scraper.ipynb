{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this code is to scrape the data and put it in a file. Docker is needed to run this code. Part of this code is retrieved from another repository of me: https://github.com/janvanderdoe/amazon_nfu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and start docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from collections import Counter\n",
    "import docker\n",
    "import re\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = docker.from_env()\n",
    "container = client.containers.get('splash')\n",
    "#start container\n",
    "container.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    t = 0\n",
    "    if url.startswith(\"https://www.amazon.com/dp/\") or url.startswith(\"https://www.amazon.com/s?i\"):\n",
    "        time_wait = 1\n",
    "    else:\n",
    "        time_wait = 20\n",
    "    while True:\n",
    "        r = requests.get(\"http://localhost:8050/render.html\", params={\"url\": url,'wait': time_wait, 'viewport': '320x480', 'images': 0, 'resource_timeout': 10}, headers={'User-Agent': user_agent})\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        #check if h4 is nonetype\n",
    "        if soup.find('h4') == None:\n",
    "            pass\n",
    "        elif soup.find('h4').text == 'Enter the characters you see below':\n",
    "            raise Exception('Captcha')\n",
    "        #print(soup.title.text)\n",
    "        if soup.text.find('GlobalTimeoutError') != -1:\n",
    "            container.restart()\n",
    "            time.sleep(31)\n",
    "            print(f\"time sleep try {t}\")\n",
    "            time_wait += 4\n",
    "            if t == 2:\n",
    "                print(f\"too many timeouts (2) for url: {url}\")\n",
    "                break\n",
    "            t += 1\n",
    "        else:\n",
    "            print(f\"page successfully loaded in time {time_wait}: {url}\")\n",
    "            break       \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product url scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_asin(link):\n",
    "    link_asin = {}\n",
    "    splitted_link = link['href'].split('/')\n",
    "    name = splitted_link[1]\n",
    "    link_asin['link'] = name\n",
    "    asin = splitted_link[3]\n",
    "    link_asin['asin'] = asin\n",
    "    #You can create the link to the review page with the name and asin\n",
    "    link_asin['full_link'] = \"https://www.amazon.com/\"+str(name)+\"/product-reviews/\"+str(asin)+\"/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber=\"\n",
    "    return link_asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f\"../../gen/input/amazon_links_per_category.csv\", \"w\", encoding = \"UTF-8\", newline = '') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \";\")\n",
    "    writer.writerow([\"name\", \"asin\", \"product_link\", \"page_link\", \"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_links = []\n",
    "category = \"computer adapter\"\n",
    "i = 12\n",
    "with open(f\"../../gen/input/amazon_links_per_category.csv\", \"a\", encoding = \"UTF-8\", newline = '') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \";\")\n",
    "    while i < 24:\n",
    "        page_link = f\"https://www.amazon.com/s?k=computer+adapter&i=electronics&rh=n%3A172282%2Cn%3A281407%2Cp_n_condition-type%3A2224371011&dc&page={i}&crid=1QAMSMU53HDFW&qid=1713623177&rnid=172282&sprefix=computer+adapter%2Caps%2C215&ref=sr_pg_{i}\"\n",
    "        soup = get_soup(page_link)\n",
    "        print(f\"currently looking at page {i}\")\n",
    "        #Block usually after about 100 pages\n",
    "        if soup.title.text==\"Sorry! Something went wrong!\":\n",
    "            print(\"Blocked\")\n",
    "            break\n",
    "        #If not block, gather asin and link\n",
    "        else:\n",
    "            links = soup.find_all(class_=\"a-link-normal s-no-outline\")\n",
    "        for link in links:\n",
    "            if link['href'].find('/dp/') != -1 and not link['href'].startswith('/dp/'):\n",
    "                link_asin = get_link_asin(link)\n",
    "                link_asin['page_link'] = page_link\n",
    "                link_asin['category'] = category\n",
    "                all_product_links.append(link_asin)\n",
    "                writer.writerow(list(link_asin.values()))\n",
    "        #Try to find next page button. If absent, break\n",
    "        try:\n",
    "            soup.find('span', class_ = \"s-pagination-item s-pagination-next s-pagination-disabled\").text\n",
    "            print('last page')\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "        print(len(all_product_links))\n",
    "        i += 1\n",
    "    #stop docker\n",
    "    container.stop()\n",
    "    container.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f\"../../data/amazon_search_hedonic.csv\", \"w\", encoding = \"UTF-8\", newline = '') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \";\")\n",
    "    writer.writerow([\"asin_url\",  \"link\", \"product_title\", \"rating\", \"review_title\", \"country\", \"date\", \"review\", \"helpful\", \"picture\",\n",
    "            \"verified\", \"video\", \"scrape_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function retrieves all the review info from a page\n",
    "def get_reviews_from_page(product_info, start=1, end = 500):\n",
    "    print(f\"succesfully loaded {product_info['title']}\")\n",
    "    with open(f\"../../data/amazon_search_hedonic.csv\", \"a\", encoding = \"UTF-8\", newline = '') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter = \";\")\n",
    "        for i in range(start,end):\n",
    "            missing_page = []\n",
    "            soup_reviews = get_soup((product_info['link']+str(i)).replace(\"ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\", f\"ref=cm_cr_getr_d_paging_btm_next_{i}?pageNumber=\"))\n",
    "            \n",
    "            #check if chaptcha\n",
    "            if soup_reviews.find('h4') == None:\n",
    "                pass\n",
    "            elif soup_reviews.find('h4').text == 'Enter the characters you see below':\n",
    "                print('chaptcha check')\n",
    "                #container.stop()\n",
    "                raise Exception('chaptcha')\n",
    "            reviews = soup_reviews.find_all('div', {\"data-hook\":\"review\"})\n",
    "            if len(reviews) == 0:\n",
    "                print(f'Could not find any reviews at page {i}')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "            for review in reviews:\n",
    "                review_info = {}\n",
    "                review_info = {\n",
    "                'product_url' : product_info['link']+str(i),\n",
    "                'link' : product_info['link']+str(i),\n",
    "                'product_title' : product_info['title']\n",
    "                }\n",
    "                try:\n",
    "                    review_info['star_rating'] = float(review.find('i', {\"data-hook\":\"review-star-rating\"}).text.replace(' out of 5 stars',''))\n",
    "                except:\n",
    "                    #foreign countries\n",
    "                    try:\n",
    "                        review_info['star_rating'] = float(review.find('i', {\"data-hook\":\"cmps-review-star-rating\"}).text.replace(' out of 5 stars',''))\n",
    "                    except:\n",
    "                        review_info['star_rating'] = None\n",
    "                try:\n",
    "                    review_info['title'] = review.find('a', {\"data-hook\":\"review-title\"}).text.strip()\n",
    "                except:\n",
    "                    try:\n",
    "                        #foreign countries\n",
    "                        review_info['title'] = review.find('span',{\"data-hook\":\"review-title\"}).text.strip()\n",
    "                    except:\n",
    "                        review_info['title'] = None\n",
    "                try:\n",
    "                    review_info['country'] = review.find('span', {\"data-hook\":\"review-date\"}).text.replace('Reviewed in ','').rpartition(' on')[0]\n",
    "                except:\n",
    "                    review_info['country'] = None\n",
    "                try:\n",
    "                    review_info['date'] = review.find('span', {\"data-hook\":\"review-date\"}).text.replace('Reviewed in ','').rpartition(' on')[2].strip()\n",
    "                except:\n",
    "                    review_info['date'] = None\n",
    "                try:\n",
    "                    span_elements = review.find('span', {\"data-hook\":\"review-body\"}).find_all('span')\n",
    "                    filtered_span_elements = [span for span in span_elements if span.get('class') is None]\n",
    "                    concatenated_text = ''.join([span.text for span in filtered_span_elements])\n",
    "                    review_info['review'] = concatenated_text\n",
    "                except:\n",
    "                    review_info['review'] = None\n",
    "                try:\n",
    "                    helpful = review.find('span', {\"data-hook\" : \"helpful-vote-statement\"}).text.strip()\n",
    "                    if helpful == \"One person found this helpful\":\n",
    "                        review_info['helpful'] = 1\n",
    "                    elif \"found this helpful\" in helpful:\n",
    "                        review_info['helpful'] = helpful.split()[0]\n",
    "                    else:\n",
    "                        review_info['helpful'] = helpful\n",
    "                except:\n",
    "                    review_info['helpful'] = None\n",
    "                try:\n",
    "                    #get links for consumer pictures\n",
    "                    if review.find('div', class_=\"review-image-tile-section\") is None:\n",
    "                        review_info['picture'] = 'no'\n",
    "                    else:\n",
    "                        pics = review.find('div', class_ = \"a-section a-spacing-top-mini cr-lightbox-image-thumbnails\").find_all('img')\n",
    "                        pics = [img['src'] for img in pics]\n",
    "                        review_info['picture'] = pics\n",
    "                        #saving pics\n",
    "                        try:\n",
    "                            for pic in pics:\n",
    "                                hq_pic = pic.replace(\"_SY88.jpg\", \"_SL1600_.jpg\")\n",
    "                                r = requests.get(hq_pic)\n",
    "                                with open(f\"../../gen/output/consumer_photos_category/\"+hq_pic.split('/')[-1], \"wb\") as f:\n",
    "                                    f.write(r.content)\n",
    "                                time.sleep(0.1)\n",
    "                        except:\n",
    "                            print(\"Error while downloading pictures\")\n",
    "                except:\n",
    "                    review_info['picture'] = None\n",
    "\n",
    "                try:\n",
    "                    if review.find('span', {\"data-hook\":\"avp-badge\"}) is None:\n",
    "                        review_info['verified'] = 'no'\n",
    "                    else:\n",
    "                        review_info['verified'] = 'yes'\n",
    "                except:\n",
    "                    review_info['verified'] = None   \n",
    "                try:\n",
    "                    review_info['video'] = review.find('input', class_=\"video-url\").get('value')\n",
    "                except:\n",
    "                    review_info['video'] = None\n",
    "                review_info['scrape_date'] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                all_reviews.append(review_info)\n",
    "                missing_values = [key for key,value in review_info.items() if value == None]\n",
    "                missing_page += missing_values\n",
    "                writer.writerow(list(review_info.values()))\n",
    "            print(len(all_reviews))\n",
    "            if len(all_reviews) % 500 == 0:\n",
    "                print('restart container')\n",
    "                container.restart()\n",
    "                time.sleep(15)\n",
    "            #missing values checker\n",
    "            if len(missing_page) > 0:\n",
    "                print(f\"the following items were missing: {dict(Counter(missing_page))}\")\n",
    "\n",
    "            if not soup_reviews.find('li', class_ = \"a-disabled a-last\"):\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"page {i} was the last page\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv using pandas\n",
    "import pandas as pd\n",
    "import re\n",
    "all_reviews = []\n",
    "#no headers\n",
    "links_data = pd.read_csv(f\"../../gen/input/amazon_links_per_category.csv\", delimiter = \";\")\n",
    "links_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_data[links_data['category'] == 'computer adapter']['page_link'][1484]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_join_name(name):\n",
    "    return '-'.join(name.split('-')[0:3])\n",
    "\n",
    "links_data['name_part'] = links_data['name'].apply(split_join_name)\n",
    "#remove duplicates in name_part\n",
    "print(len(links_data))\n",
    "links_data = links_data.drop_duplicates(subset='name_part', keep='first')\n",
    "print(len(links_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_so_far = pd.read_csv(f\"../../data/amazon_search_hedonic.csv\", delimiter = \";\")\n",
    "data_so_far['product_title_clean'] = data_so_far['product_title'].apply(split_join_name)\n",
    "#only keep links_data rows which name_part is not in product_title_clean\n",
    "links_data = links_data[~links_data['name_part'].isin(data_so_far['product_title_clean'])]\n",
    "print(len(links_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_digit(string):\n",
    "    return re.sub(r'\\d+$', '', string)\n",
    "\n",
    "\n",
    "data_so_far['asin_url_clean'] = data_so_far['asin_url'].apply(remove_last_digit)\n",
    "#merge with links_data\n",
    "links_data_full = pd.read_csv(f\"../../gen/input/amazon_links_per_category.csv\", delimiter = \";\")\n",
    "data_so_far = pd.merge(data_so_far, links_data_full, left_on = 'asin_url_clean', right_on = 'product_link', how = 'left')\n",
    "print(data_so_far['category'].value_counts())\n",
    "print(\"----------------------\")\n",
    "#remove duplicates for link, product_title, review_title, review, date, name_x\n",
    "data_so_far = data_so_far.drop_duplicates(subset = ['review_title', 'review', 'date', 'name_x', 'profile_url'])\n",
    "\n",
    "#remove obs not from the united states\n",
    "data_so_far = data_so_far[data_so_far['country'] == 'the United States']\n",
    "data_so_far = data_so_far[data_so_far['review'] != '']\n",
    "#show number of observations per category_y\n",
    "print(data_so_far['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = []\n",
    "\n",
    "#filter for dvd_player\n",
    "links_data_filtered = links_data[links_data['category'] == 'smartphone']\n",
    "links_so_far = data_so_far['asin_url'].unique()\n",
    "print(len(links_data_filtered))\n",
    "#remove last digits in links_so_far\n",
    "links_so_far = [re.sub(r'\\d+$', '', link) for link in links_so_far]\n",
    "links_left = [links for links in links_data_filtered['product_link'] if links not in links_so_far]\n",
    "print(len(links_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "import random\n",
    "random.seed(42)\n",
    "#shuffle links\n",
    "random.shuffle(links_left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1\n",
    "scrape_links = links_left[100:200]\n",
    "for link in scrape_links:\n",
    "    print(f\"{l} / {len(scrape_links)}\")\n",
    "    product_info = {\n",
    "    'product_url' : \"https://www.amazon.com/dp/\"+link.split('/')[-2],\n",
    "    'brand' : None,\n",
    "    'model' : None,\n",
    "    'link' : link,\n",
    "    'title' : link.split('/')[3],\n",
    "    'product_title' : None\n",
    "    }\n",
    "    get_reviews_from_page(product_info, 1, 13)\n",
    "    l += 1\n",
    "    if l % 10 == 0:\n",
    "        print('restart container')\n",
    "        container.restart()\n",
    "        time.sleep(10)\n",
    "container.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## consumer photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_photos = data['picture'].unique()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def get_links(pics):\n",
    "    list_pics = ast.literal_eval(pics)\n",
    "    return [pic.split('/')[-1] for pic in list_pics]\n",
    "\n",
    "#apply to consumer)photos\n",
    "consumer_photos_clean = [get_links(pics) for pics in consumer_photos]\n",
    "consumer_photos_clean = [item for sublist in consumer_photos_clean for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all existing consumer photos\n",
    "consumer_photos = os.listdir(\"../../gen/output/consumer_photos_category\")\n",
    "cp_left = [cp for cp in consumer_photos_clean if cp.replace('_SY88.jpg', '_SL1600_.jpg') not in consumer_photos]\n",
    "cp_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape consumer photos\n",
    "for cp in cp_left:\n",
    "    try:\n",
    "        cp = cp.replace('_SY88.jpg', '_SL1600_.jpg')\n",
    "        r = requests.get(\"https://images-na.ssl-images-amazon.com/images/I/\"+cp)\n",
    "        with open(f\"../../gen/output/consumer_photos_category/\"+cp, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        time.sleep(0.1)\n",
    "    except:\n",
    "        print(f\"error with {cp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
